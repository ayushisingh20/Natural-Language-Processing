{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# spaCy"
      ],
      "metadata": {
        "id": "qlukcXzhuO7z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jS-SFHWUt509"
      },
      "outputs": [],
      "source": [
        "import spacy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\") # the spaCy English language model\n"
      ],
      "metadata": {
        "id": "Rew2ECxVuXrV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"My name is Dr. Ayushi. I am a student of compter science. I loved mathematics.\")"
      ],
      "metadata": {
        "id": "QgFpoc6Huj74"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for sentence in doc.sents:  # sentence tokenisation\n",
        "  print(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fDILcOzuvBw3",
        "outputId": "d368defc-3b41-4953-b92e-430cba5c8da3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "My name is Dr. Ayushi.\n",
            "I am a student of compter science.\n",
            "I loved mathematics\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for sentence in doc.sents: # word tokenisation\n",
        "  for word in sentence:\n",
        "    print(word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z4he9ZUYvMOg",
        "outputId": "00745558-8daf-4de5-fba7-245467376b85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "My\n",
            "name\n",
            "is\n",
            "Dr.\n",
            "Ayushi\n",
            ".\n",
            "I\n",
            "am\n",
            "a\n",
            "student\n",
            "of\n",
            "compter\n",
            "science\n",
            ".\n",
            "I\n",
            "loved\n",
            "mathematics\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NLTK"
      ],
      "metadata": {
        "id": "501Z0Aiovvoy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk"
      ],
      "metadata": {
        "id": "S8SIikASvsE8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jmBXSuU_xV24",
        "outputId": "6da1a506-50f9-458a-e8fd-e5857f07105a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "text = \"My name is Dr. Ayushi. I am a student of computer science. I love mathematics.\"\n",
        "\n",
        "# Tokenize the text into sentences\n",
        "sentences = sent_tokenize(text)\n",
        "print(sentences)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5dkEbsPv5pa",
        "outputId": "a75031d9-800a-469f-97f9-fdd1be2c33bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['My name is Dr. Ayushi.', 'I am a student of computer science.', 'I love mathematics.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Print the result\n",
        "for sentence in sentences:\n",
        "    print(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7JP1eSKPwQd4",
        "outputId": "4f68712a-4190-4de9-87ab-02b71f11f101"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "My name is Dr. Ayushi.\n",
            "I am a student of computer science.\n",
            "I love mathematics.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "JGtFdtMexeZ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"My name is Dr. Ayushi. I am a student of computer science. I love mathematics.\"\n",
        "\n",
        "# Tokenize the text into words\n",
        "words = word_tokenize(text)\n",
        "for sentence in sentences:\n",
        "  for word in sentence:\n",
        "    print(word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u9C0_t-UyD1a",
        "outputId": "63d8c897-7ddc-4f70-9335-18160dbdeb80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "M\n",
            "y\n",
            " \n",
            "n\n",
            "a\n",
            "m\n",
            "e\n",
            " \n",
            "i\n",
            "s\n",
            " \n",
            "D\n",
            "r\n",
            ".\n",
            " \n",
            "A\n",
            "y\n",
            "u\n",
            "s\n",
            "h\n",
            "i\n",
            ".\n",
            "I\n",
            " \n",
            "a\n",
            "m\n",
            " \n",
            "a\n",
            " \n",
            "s\n",
            "t\n",
            "u\n",
            "d\n",
            "e\n",
            "n\n",
            "t\n",
            " \n",
            "o\n",
            "f\n",
            " \n",
            "c\n",
            "o\n",
            "m\n",
            "p\n",
            "u\n",
            "t\n",
            "e\n",
            "r\n",
            " \n",
            "s\n",
            "c\n",
            "i\n",
            "e\n",
            "n\n",
            "c\n",
            "e\n",
            ".\n",
            "I\n",
            " \n",
            "l\n",
            "o\n",
            "v\n",
            "e\n",
            " \n",
            "m\n",
            "a\n",
            "t\n",
            "h\n",
            "e\n",
            "m\n",
            "a\n",
            "t\n",
            "i\n",
            "c\n",
            "s\n",
            ".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenisation in Spacy"
      ],
      "metadata": {
        "id": "2-k1Nwkt-nbG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n"
      ],
      "metadata": {
        "id": "eO8tGNynyLVS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# we are creating a blank English language model using spaCy.\n",
        "\n",
        "nlp = spacy.blank(\"en\") # language object # en ---> english"
      ],
      "metadata": {
        "id": "u8HAlyF4IRj-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"My name is Dr. Ayushi. I am a student of computer science. I loved mathematics.\")"
      ],
      "metadata": {
        "id": "J7iDllDNIYwT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for token in doc:\n",
        "  print(token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8HQZauKzKPFs",
        "outputId": "a4b50589-78ed-49ac-a7a2-87155d057734"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "My\n",
            "name\n",
            "is\n",
            "Dr.\n",
            "Ayushi\n",
            ".\n",
            "I\n",
            "am\n",
            "a\n",
            "student\n",
            "of\n",
            "computer\n",
            "science\n",
            ".\n",
            "I\n",
            "loved\n",
            "mathematics\n",
            ".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc[0] #index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21MBE9XBKWtS",
        "outputId": "937b2cdb-86fe-45e9-9256-deef9259cb00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "My"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc1 = nlp(''' \"let's go to N.Y.!\" ''')\n",
        "for token in doc1:\n",
        "  print(token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_u_Hq8UlLJWd",
        "outputId": "3864ffed-9ddb-4257-875d-9cb02fdab395"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " \n",
            "\"\n",
            "let\n",
            "'s\n",
            "go\n",
            "to\n",
            "N.Y.\n",
            "!\n",
            "\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(nlp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XMX6BMhyMKya",
        "outputId": "2a423b5b-189e-4ee8-f839-89bfb7f26074"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "spacy.lang.en.English"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(doc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I9J3HpJTMa5n",
        "outputId": "1c0c0b9e-c65b-4e67-8031-2e64d4b292f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "spacy.tokens.doc.Doc"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WptliLWYMcws",
        "outputId": "e963d3c8-fe1c-4e1b-ea3f-5c82d7907656"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "spacy.tokens.token.Token"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc2 = nlp(\"Tony gave two $ to Samay\")"
      ],
      "metadata": {
        "id": "Uoa-pznNMiSo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token0 = doc2[0]\n",
        "print(token0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZcQv-xqMzjs",
        "outputId": "fdae4e0f-ab0e-493b-cf39-e92de1116d04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tony\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dir(token0)\n",
        "\n",
        "# In the context of spaCy, if you want to explore the available attributes and methods of a Token object, you can use dir() on that object.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x25uNE9FM_xk",
        "outputId": "3ec9e3e6-e6e1-4c0a-b258-a6f27191009a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['_',\n",
              " '__bytes__',\n",
              " '__class__',\n",
              " '__delattr__',\n",
              " '__dir__',\n",
              " '__doc__',\n",
              " '__eq__',\n",
              " '__format__',\n",
              " '__ge__',\n",
              " '__getattribute__',\n",
              " '__gt__',\n",
              " '__hash__',\n",
              " '__init__',\n",
              " '__init_subclass__',\n",
              " '__le__',\n",
              " '__len__',\n",
              " '__lt__',\n",
              " '__ne__',\n",
              " '__new__',\n",
              " '__pyx_vtable__',\n",
              " '__reduce__',\n",
              " '__reduce_ex__',\n",
              " '__repr__',\n",
              " '__setattr__',\n",
              " '__sizeof__',\n",
              " '__str__',\n",
              " '__subclasshook__',\n",
              " '__unicode__',\n",
              " 'ancestors',\n",
              " 'check_flag',\n",
              " 'children',\n",
              " 'cluster',\n",
              " 'conjuncts',\n",
              " 'dep',\n",
              " 'dep_',\n",
              " 'doc',\n",
              " 'ent_id',\n",
              " 'ent_id_',\n",
              " 'ent_iob',\n",
              " 'ent_iob_',\n",
              " 'ent_kb_id',\n",
              " 'ent_kb_id_',\n",
              " 'ent_type',\n",
              " 'ent_type_',\n",
              " 'get_extension',\n",
              " 'has_dep',\n",
              " 'has_extension',\n",
              " 'has_head',\n",
              " 'has_morph',\n",
              " 'has_vector',\n",
              " 'head',\n",
              " 'i',\n",
              " 'idx',\n",
              " 'iob_strings',\n",
              " 'is_alpha',\n",
              " 'is_ancestor',\n",
              " 'is_ascii',\n",
              " 'is_bracket',\n",
              " 'is_currency',\n",
              " 'is_digit',\n",
              " 'is_left_punct',\n",
              " 'is_lower',\n",
              " 'is_oov',\n",
              " 'is_punct',\n",
              " 'is_quote',\n",
              " 'is_right_punct',\n",
              " 'is_sent_end',\n",
              " 'is_sent_start',\n",
              " 'is_space',\n",
              " 'is_stop',\n",
              " 'is_title',\n",
              " 'is_upper',\n",
              " 'lang',\n",
              " 'lang_',\n",
              " 'left_edge',\n",
              " 'lefts',\n",
              " 'lemma',\n",
              " 'lemma_',\n",
              " 'lex',\n",
              " 'lex_id',\n",
              " 'like_email',\n",
              " 'like_num',\n",
              " 'like_url',\n",
              " 'lower',\n",
              " 'lower_',\n",
              " 'morph',\n",
              " 'n_lefts',\n",
              " 'n_rights',\n",
              " 'nbor',\n",
              " 'norm',\n",
              " 'norm_',\n",
              " 'orth',\n",
              " 'orth_',\n",
              " 'pos',\n",
              " 'pos_',\n",
              " 'prefix',\n",
              " 'prefix_',\n",
              " 'prob',\n",
              " 'rank',\n",
              " 'remove_extension',\n",
              " 'right_edge',\n",
              " 'rights',\n",
              " 'sent',\n",
              " 'sent_start',\n",
              " 'sentiment',\n",
              " 'set_extension',\n",
              " 'set_morph',\n",
              " 'shape',\n",
              " 'shape_',\n",
              " 'similarity',\n",
              " 'subtree',\n",
              " 'suffix',\n",
              " 'suffix_',\n",
              " 'tag',\n",
              " 'tag_',\n",
              " 'tensor',\n",
              " 'text',\n",
              " 'text_with_ws',\n",
              " 'vector',\n",
              " 'vector_norm',\n",
              " 'vocab',\n",
              " 'whitespace_']"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token0.is_alpha  # is alphabetic"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8pQ4ePrNEOc",
        "outputId": "c07d7bf9-ba83-42f1-df0d-67eac44c1f9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token0.like_num"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L7GEN3cINXkw",
        "outputId": "8b36f5e5-1357-44a5-8ff0-23c5a1639d6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token2 = doc2[2]\n",
        "token2.text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "FX_K_1XeNbZr",
        "outputId": "960ae725-9a75-4052-dd1a-becd8e93a17a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'two'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token2.like_num # is like a number ?"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5pJJlEx3Nsbx",
        "outputId": "6baaf502-73d4-44b7-eda0-415bb1de555a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token3 = doc2[3]\n",
        "token3.text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "QkaNMqo7N0Sj",
        "outputId": "8f8f3241-7d31-43a8-f910-7cc8f76fb130"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'$'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token3.is_currency # is liike currency ?"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0zcSwSF4N74e",
        "outputId": "48665a1a-0ec2-4b4e-ea46-2b5607c9bc05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Collecting email ids of students from students information sheet"
      ],
      "metadata": {
        "id": "Z_1tui0DPBfz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"studentdata.rtf\") as f:\n",
        "  text=f.readlines()"
      ],
      "metadata": {
        "id": "YUWFhRMrOCRR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xKGMtRkERR6t",
        "outputId": "72f541ed-256e-4c96-f3b0-28fd59956b6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['{\\\\rtf1\\\\ansi\\\\ansicpg1252\\\\cocoartf2708\\n', '\\\\cocoatextscaling0\\\\cocoaplatform0{\\\\fonttbl\\\\f0\\\\fswiss\\\\fcharset0 Helvetica;}\\n', '{\\\\colortbl;\\\\red255\\\\green255\\\\blue255;}\\n', '{\\\\*\\\\expandedcolortbl;;}\\n', '\\\\paperw11900\\\\paperh16840\\\\margl1440\\\\margr1440\\\\vieww11520\\\\viewh8400\\\\viewkind0\\n', '\\\\pard\\\\tx720\\\\tx1440\\\\tx2160\\\\tx2880\\\\tx3600\\\\tx4320\\\\tx5040\\\\tx5760\\\\tx6480\\\\tx7200\\\\tx7920\\\\tx8640\\\\pardirnatural\\\\partightenfactor0\\n', '\\n', '\\\\f0\\\\fs24 \\\\cf0 Sunrise High School , Grade 8 Students information \\\\\\n', '========================================\\\\\\n', '\\\\\\n', 'Name             Birthday                      Email-id \\\\\\n', \"\\\\'97\\\\'97\\\\'97            \\\\'97\\\\'97\\\\'97\\\\'97                    \\\\'97\\\\'97\\\\'97\\\\'97\\\\\\n\", '\\\\\\n', 'Aru                 2,June 2000                aru123@gmail.com\\\\\\n', 'Charu             5,July 2001                 charusao@gmail.com\\\\\\n', 'Sonam           17,Feb 2002                sonam@yahoo.com\\\\\\n', 'John              19, December 2022     johnjohn@gmail.com}']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8MdRUp5RZRF",
        "outputId": "41102682-f528-463e-ded7-3aca72377217"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['{\\\\rtf1\\\\ansi\\\\ansicpg1252\\\\cocoartf2708\\n',\n",
              " '\\\\cocoatextscaling0\\\\cocoaplatform0{\\\\fonttbl\\\\f0\\\\fswiss\\\\fcharset0 Helvetica;}\\n',\n",
              " '{\\\\colortbl;\\\\red255\\\\green255\\\\blue255;}\\n',\n",
              " '{\\\\*\\\\expandedcolortbl;;}\\n',\n",
              " '\\\\paperw11900\\\\paperh16840\\\\margl1440\\\\margr1440\\\\vieww11520\\\\viewh8400\\\\viewkind0\\n',\n",
              " '\\\\pard\\\\tx720\\\\tx1440\\\\tx2160\\\\tx2880\\\\tx3600\\\\tx4320\\\\tx5040\\\\tx5760\\\\tx6480\\\\tx7200\\\\tx7920\\\\tx8640\\\\pardirnatural\\\\partightenfactor0\\n',\n",
              " '\\n',\n",
              " '\\\\f0\\\\fs24 \\\\cf0 Sunrise High School , Grade 8 Students information \\\\\\n',\n",
              " '========================================\\\\\\n',\n",
              " '\\\\\\n',\n",
              " 'Name             Birthday                      Email-id \\\\\\n',\n",
              " \"\\\\'97\\\\'97\\\\'97            \\\\'97\\\\'97\\\\'97\\\\'97                    \\\\'97\\\\'97\\\\'97\\\\'97\\\\\\n\",\n",
              " '\\\\\\n',\n",
              " 'Aru                 2,June 2000                aru123@gmail.com\\\\\\n',\n",
              " 'Charu             5,July 2001                 charusao@gmail.com\\\\\\n',\n",
              " 'Sonam           17,Feb 2002                sonam@yahoo.com\\\\\\n',\n",
              " 'John              19, December 2022     johnjohn@gmail.com}']"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = ' '.join(text)"
      ],
      "metadata": {
        "id": "UeRZqal8RbvJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "Nmakh8fDRneD",
        "outputId": "5ae2bcbe-30b6-4c90-9b88-924d958b3cf8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"{\\\\rtf1\\\\ansi\\\\ansicpg1252\\\\cocoartf2708\\n \\\\cocoatextscaling0\\\\cocoaplatform0{\\\\fonttbl\\\\f0\\\\fswiss\\\\fcharset0 Helvetica;}\\n {\\\\colortbl;\\\\red255\\\\green255\\\\blue255;}\\n {\\\\*\\\\expandedcolortbl;;}\\n \\\\paperw11900\\\\paperh16840\\\\margl1440\\\\margr1440\\\\vieww11520\\\\viewh8400\\\\viewkind0\\n \\\\pard\\\\tx720\\\\tx1440\\\\tx2160\\\\tx2880\\\\tx3600\\\\tx4320\\\\tx5040\\\\tx5760\\\\tx6480\\\\tx7200\\\\tx7920\\\\tx8640\\\\pardirnatural\\\\partightenfactor0\\n \\n \\\\f0\\\\fs24 \\\\cf0 Sunrise High School , Grade 8 Students information \\\\\\n ========================================\\\\\\n \\\\\\n Name             Birthday                      Email-id \\\\\\n \\\\'97\\\\'97\\\\'97            \\\\'97\\\\'97\\\\'97\\\\'97                    \\\\'97\\\\'97\\\\'97\\\\'97\\\\\\n \\\\\\n Aru                 2,June 2000                aru123@gmail.com\\\\\\n Charu             5,July 2001                 charusao@gmail.com\\\\\\n Sonam           17,Feb 2002                sonam@yahoo.com\\\\\\n John              19, December 2022     johnjohn@gmail.com}\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc4 = nlp(text)\n",
        "emails = []\n",
        "\n",
        "for token in doc4:\n",
        "    if token.like_email:\n",
        "        # Remove trailing backslashes from the email addresses\n",
        "        clean_email = token.text.rstrip('\\\\')\n",
        "        emails.append(clean_email)\n",
        "\n",
        "print(emails)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KXik2HlhRorE",
        "outputId": "c565cb94-d044-466a-a86e-53282e76b0d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['aru123@gmail.com', 'charusao@gmail.com', 'sonam@yahoo.com', 'johnjohn@gmail.com']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Customizing tokenizer"
      ],
      "metadata": {
        "id": "t3hpc9uET929"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.symbols import ORTH\n",
        "\n",
        "nlp = spacy.blank(\"en\")\n",
        "doc9 = nlp(\"gimme double cheese extra large healthy pizza\")\n",
        "tokens = [token.text for token in doc9]\n",
        "tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y3LgzWtTSKqo",
        "outputId": "a122e5e8-1ff8-4da9-e22d-2024b0d0fa62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['gimme', 'double', 'cheese', 'extra', 'large', 'healthy', 'pizza']"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# gimme ----> give and me ---> not allowed to modify the real text you can only split it.\n",
        "\n",
        "# you've customized the tokenizer to treat \"gimme\" in a specific way, and when you tokenize a sentence containing \"gimme,\"\n",
        "# it breaks it into two separate tokens according to your specified rules."
      ],
      "metadata": {
        "id": "axFXzcSTUPI3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp.tokenizer.add_special_case(\"gimme\", [\n",
        "    {ORTH: \"gim\"},\n",
        "    {ORTH: \"me\"},\n",
        "])\n",
        "doc = nlp(\"gimme double cheese extra large healthy pizza\")\n",
        "tokens = [token.text for token in doc]\n",
        "tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7OWLfCAbU5LH",
        "outputId": "6a10aece-195c-48f4-c19c-12d9a5ebaac6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['gim', 'me', 'double', 'cheese', 'extra', 'large', 'healthy', 'pizza']"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sentence Tokenization or Segmentation"
      ],
      "metadata": {
        "id": "vT-eKkblVb4F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"Dr. Strange loves pav bhaji of mumbai. Hulk loves chat of delhi\")\n",
        "for sentence in doc.sents:\n",
        "    print(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "id": "gWeHuQLcVBZ6",
        "outputId": "e1629417-3879-4fa8-b739-c1edf1d25e8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "[E030] Sentence boundaries unset. You can add the 'sentencizer' component to the pipeline with: `nlp.add_pipe('sentencizer')`. Alternatively, add the dependency parser or sentence recognizer, or set sentence boundaries by setting `doc[i].is_sent_start`.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-82-220059dad4ce>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Dr. Strange loves pav bhaji of mumbai. Hulk loves chat of delhi\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/tokens/doc.pyx\u001b[0m in \u001b[0;36msents\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: [E030] Sentence boundaries unset. You can add the 'sentencizer' component to the pipeline with: `nlp.add_pipe('sentencizer')`. Alternatively, add the dependency parser or sentence recognizer, or set sentence boundaries by setting `doc[i].is_sent_start`."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp.pipeline"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GY7I6wB0WML3",
        "outputId": "91a1d984-1f4f-42a3-9b2e-80078f40e700"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp.add_pipe('sentencizer')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qFzQFk6lWSt8",
        "outputId": "fdb02dc0-b12f-4827-e877-67a4a47cf30c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<spacy.pipeline.sentencizer.Sentencizer at 0x7d3681bf7bc0>"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"Dr. Strange loves pav bhaji of mumbai. Hulk loves chat of delhi\")\n",
        "for sentence in doc.sents:\n",
        "    print(sentence)"
      ],
      "metadata": {
        "id": "XXBEmWn1WUub",
        "outputId": "d6e06c33-88ba-45a6-d933-c0964e5b8332",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dr. Strange loves pav bhaji of mumbai.\n",
            "Hulk loves chat of delhi\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp.pipeline"
      ],
      "metadata": {
        "id": "HglHSjIwWXDT",
        "outputId": "d2943e9f-f5ed-4727-ffdb-f0e1614df616",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('sentencizer', <spacy.pipeline.sentencizer.Sentencizer at 0x7d3681bf7bc0>)]"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WPSrlDl-WZqp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}